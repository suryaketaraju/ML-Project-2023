{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6974755,"sourceType":"datasetVersion","datasetId":3958463},{"sourceId":7072708,"sourceType":"datasetVersion","datasetId":4073315}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nimport os\nimport torch\n\n# Load tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # Assign end-of-text as the padding token\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# Load and process the Ramayana text data\nramayana_train_file = '/kaggle/input/ramayana-processed/ramayana train.txt'  \n\nwith open(ramayana_train_file, 'r', encoding='utf-8') as file:\n    ramayana_text = file.readlines()\n\nvalidation_file_path = '/kaggle/input/ramayana-processed/validation.txt'  \n\nwith open(validation_file_path, 'r', encoding='utf-8') as file:\n    validation_text = file.read()\n\nvalidation_dir = '/kaggle/working/validation/'  \nos.makedirs(validation_dir, exist_ok=True)\n\nvalidation_file_path = os.path.join(validation_dir, 'validation_ramayana.txt')\nwith open(validation_file_path, 'w', encoding='utf-8') as file:\n    file.write(validation_text)\n\nclass RamayanaDataset(torch.utils.data.Dataset):\n    def __init__(self, tokenizer, data):\n        self.examples = tokenizer.batch_encode_plus(\n            data,\n            padding=\"max_length\",\n            max_length=128,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n    def __len__(self):\n        return len(self.examples[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.examples[\"input_ids\"][idx],\n            \"labels\": self.examples[\"input_ids\"][idx].clone()  # For language modeling tasks\n        }\n\ntrain_dataset = RamayanaDataset(tokenizer, ramayana_text)\n\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./ramayana_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2,\n    logging_dir='./logs',\n    prediction_loss_only=True,\n    report_to=\"none\",\n    eval_steps=500,  \n    evaluation_strategy=\"steps\"\n)\n\n\nimport pickle\npickle.dump(rf,open(\"rf.h5\",\"wb\"))\n\nmodel = pickle.load(open(\"/kaggle/input/samskrta/samskrta.h5\",\"rb\"))\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n    train_dataset=train_dataset,\n    eval_dataset=RamayanaDataset(tokenizer, [validation_text])  # Use the validation text as a single example\n)\n\ntrainer.train()\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"./ramayana_model\")  # Replace with your trained model directory\n\n# Text prompt for generating predictions\ntext_prompt = \"तपःस्वाध्यायनिरतं\"\n# Tokenize the text prompt\ninput_ids = tokenizer.encode(text_prompt, return_tensors=\"pt\")\n\n# Ensure attention_mask is set for reliable results\nattention_mask = torch.ones_like(input_ids)\n\n# Generate predictions\nwith torch.no_grad():\n    output = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=150,\n        temperature=0.9,\n        pad_token_id=tokenizer.eos_token_id  # Set pad_token_id to eos_token_id\n    )\n\n#Downloaded the trained model with its weights\nimport pickle\n\nPkl_Filename = \"samskrta ocr.pkl\"  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(model, file)\n\n from IPython.display import FileLink  \n\nFileLink(r'samskrta.h5')","metadata":{"_uuid":"79f61bc6-44e7-4934-acc7-b82b0fb718d7","_cell_guid":"91953edf-0be6-41f6-b549-3e39161823d7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}