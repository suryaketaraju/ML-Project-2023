{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, Trainer, TrainingArguments\nimport os\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:21:52.164395Z","iopub.execute_input":"2023-11-08T07:21:52.164790Z","iopub.status.idle":"2023-11-08T07:22:07.551438Z","shell.execute_reply.started":"2023-11-08T07:21:52.164755Z","shell.execute_reply":"2023-11-08T07:22:07.550228Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.mask_token = None  # Set the mask token to None\n\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:22:16.922291Z","iopub.execute_input":"2023-11-08T07:22:16.922688Z","iopub.status.idle":"2023-11-08T07:22:32.009178Z","shell.execute_reply.started":"2023-11-08T07:22:16.922655Z","shell.execute_reply":"2023-11-08T07:22:32.007897Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a92a0f4f6c20423f9f2a74c9ed17464b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40dfb7bcd7e84228a7ef200824e3f2e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f7e8e9dbdc647f187c072a677faaaaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88bbb6d94d740209347ec22269bbd9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dee9f50b50445f0af2e28610890c59f"}},"metadata":{}}]},{"cell_type":"code","source":"\n# Load and process the Ramayana text data\nramayana_text_file = '/kaggle/input/sampurna-ramayanam/ramayana.txt'\n\nwith open(ramayana_text_file, 'r', encoding='utf-8') as file:\n    ramayana_text = file.read()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:28:10.288076Z","iopub.execute_input":"2023-11-08T07:28:10.288471Z","iopub.status.idle":"2023-11-08T07:28:10.326196Z","shell.execute_reply.started":"2023-11-08T07:28:10.288443Z","shell.execute_reply":"2023-11-08T07:28:10.325030Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n# Define the directory for training files\ntraining_dir = '/kaggle/working/'\nos.makedirs(training_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:28:18.724735Z","iopub.execute_input":"2023-11-08T07:28:18.725185Z","iopub.status.idle":"2023-11-08T07:28:18.730322Z","shell.execute_reply.started":"2023-11-08T07:28:18.725150Z","shell.execute_reply":"2023-11-08T07:28:18.729241Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n# Save the Ramayana text as a training file\ntraining_file_path = os.path.join(training_dir, 'ramayana.txt')\nwith open(training_file_path, 'w', encoding='utf-8') as file:\n    file.write(ramayana_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:28:26.007340Z","iopub.execute_input":"2023-11-08T07:28:26.007715Z","iopub.status.idle":"2023-11-08T07:28:26.025469Z","shell.execute_reply.started":"2023-11-08T07:28:26.007685Z","shell.execute_reply":"2023-11-08T07:28:26.024140Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n# Create a TextDataset\ntrain_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=training_file_path,\n    block_size=128  # Adjust the block_size according to your data length\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:31:53.581533Z","iopub.execute_input":"2023-11-08T07:31:53.582006Z","iopub.status.idle":"2023-11-08T07:31:53.709111Z","shell.execute_reply.started":"2023-11-08T07:31:53.581970Z","shell.execute_reply":"2023-11-08T07:31:53.707885Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\n# Define the training arguments for causal language modeling\ntraining_args = TrainingArguments(\n    output_dir=\"./ramayana_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2,\n    logging_dir='./logs',\n    prediction_loss_only=True,\n    report_to=\"none\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:31:55.021835Z","iopub.execute_input":"2023-11-08T07:31:55.022227Z","iopub.status.idle":"2023-11-08T07:31:55.028542Z","shell.execute_reply.started":"2023-11-08T07:31:55.022192Z","shell.execute_reply":"2023-11-08T07:31:55.027264Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def my_data_collator(examples):\n    input_ids = [example['input_ids'] for example in examples]\n\n    # Determine the maximum length among input_ids\n    max_length = max(len(ids) for ids in input_ids)\n\n    # Pad the input_ids directly\n    padded_input_ids = [ids + [tokenizer.pad_token_id] * (max_length - len(ids)) for ids in input_ids]\n\n    return {\n        'input_ids': torch.tensor(padded_input_ids, dtype=torch.long)\n    }\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:41:25.517226Z","iopub.execute_input":"2023-11-08T07:41:25.517643Z","iopub.status.idle":"2023-11-08T07:41:25.524055Z","shell.execute_reply.started":"2023-11-08T07:41:25.517610Z","shell.execute_reply":"2023-11-08T07:41:25.523029Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Start the training process with the custom data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=my_data_collator,\n    train_dataset=train_dataset\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:41:27.649594Z","iopub.execute_input":"2023-11-08T07:41:27.650003Z","iopub.status.idle":"2023-11-08T07:41:27.659198Z","shell.execute_reply.started":"2023-11-08T07:41:27.649970Z","shell.execute_reply":"2023-11-08T07:41:27.658190Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:41:30.029102Z","iopub.execute_input":"2023-11-08T07:41:30.029477Z","iopub.status.idle":"2023-11-08T07:41:30.585950Z","shell.execute_reply.started":"2023-11-08T07:41:30.029448Z","shell.execute_reply":"2023-11-08T07:41:30.584568Z"},"trusted":true},"execution_count":39,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1813\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1810\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1813\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1814\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_utils.py:707\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[1;32m    706\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[37], line 2\u001b[0m, in \u001b[0;36mmy_data_collator\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_data_collator\u001b[39m(examples):\n\u001b[0;32m----> 2\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m [example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Determine the maximum length among input_ids\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m input_ids)\n","Cell \u001b[0;32mIn[37], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_data_collator\u001b[39m(examples):\n\u001b[0;32m----> 2\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m [\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Determine the maximum length among input_ids\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m input_ids)\n","\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"],"ename":"IndexError","evalue":"too many indices for tensor of dimension 1","output_type":"error"}]},{"cell_type":"code","source":"import re\n\n# Example verses with potential non-Devanagari characters\nverses = [\"\n    % Ramayana: Balakanda\n% Last updated: Thu Oct 21 2021\n% Encoding: Unicode Devanagari\n%\n1001001a तपःस्वाध्यायनिरतं तपस्वी वाग्विदां वरम्\n1001001c नारदं परिपप्रच्छ वाल्मीकिर्मुनिपुंगवम्\n1001002a को न्वस्मिन्साम्प्रतं लोके गुणवान्कश्च वीर्यवान्\n1001002c धर्मज्ञश्च कृतज्ञश्च सत्यवाक्यो दृढव्रतः\n1001003a चारित्रेण च को युक्तः सर्वभूतेषु को हितः\n1001003c विद्वान्कः कः समर्थश्च कश्चैकप्रियदर्शनः\n1001004a आत्मवान्को जितक्रोधो द्युतिमान्कोऽनसूयकः\"\n]\n\n# Regular expression to filter only Devanagari characters\ndevanagari_pattern = re.compile(\"[\\u0900-\\u097F]+\")\n\n# Filter non-Devanagari characters from the verses\ndevanagari_verses = [\"\".join(re.findall(devanagari_pattern, verse)) for verse in verses]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:45:57.591697Z","iopub.execute_input":"2023-11-08T07:45:57.592573Z","iopub.status.idle":"2023-11-08T07:45:57.602600Z","shell.execute_reply.started":"2023-11-08T07:45:57.592537Z","shell.execute_reply":"2023-11-08T07:45:57.601105Z"},"trusted":true},"execution_count":42,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[42], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    1001001a तपःस्वाध्यायनिरतं तपस्वी वाग्विदां वरम्\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"],"ename":"SyntaxError","evalue":"invalid decimal literal (2278479223.py, line 9)","output_type":"error"}]},{"cell_type":"code","source":"\n# Display the filtered verses\nfor verse in devanagari_verses:\n    print(verse)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T07:45:34.526899Z","iopub.execute_input":"2023-11-08T07:45:34.527320Z","iopub.status.idle":"2023-11-08T07:45:34.533084Z","shell.execute_reply.started":"2023-11-08T07:45:34.527276Z","shell.execute_reply":"2023-11-08T07:45:34.531985Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"तपःस्वाध्यायनिरतंतपस्वीवाग्विदांवरम्\n\nनारदंपरिपप्रच्छवाल्मीकिर्मुनिपुंगवम्\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}